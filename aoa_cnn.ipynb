{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape:  (14663, 1, 312)\n",
      "train_labels shape:  (14663, 1)\n",
      "test_data shape:  (3666, 1, 312)\n",
      "test_labels shape:  (3666, 1)\n",
      "Batch  0\n",
      "Test accuracy:  0.524247491638796\n",
      "Batch  1\n",
      "Test accuracy:  0.542386185243328\n",
      "Total test accuracy:  0.533316838441062\n",
      "total test samples: 3666\n",
      "Error: 0, count: 1945, ratio: 53.05510092744135%\n",
      "Error: 1, count: 881, ratio: 24.0316421167485%\n",
      "Error: 2, count: 367, ratio: 10.010911074740863%\n",
      "Error: 3, count: 167, ratio: 4.555373704309875%\n",
      "Error: 4, count: 49, ratio: 1.336606655755592%\n",
      "Error: 5, count: 157, ratio: 4.2825968357883255%\n",
      "Error: 6, count: 5, ratio: 0.13638843426077468%\n",
      "Error: 7, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 8, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 10, count: 21, ratio: 0.5728314238952537%\n",
      "Error: 11, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 12, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 13, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 14, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 15, count: 9, ratio: 0.24549918166939444%\n",
      "Error: 17, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 18, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 20, count: 6, ratio: 0.16366612111292964%\n",
      "Error: 21, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 22, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 25, count: 6, ratio: 0.16366612111292964%\n",
      "Error: 27, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 30, count: 4, ratio: 0.10911074740861974%\n",
      "Error: 33, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 35, count: 3, ratio: 0.08183306055646482%\n",
      "Error: 36, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 40, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 45, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 47, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 54, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 59, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 60, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 63, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 66, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 67, count: 4, ratio: 0.10911074740861974%\n",
      "Error: 68, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 69, count: 4, ratio: 0.10911074740861974%\n",
      "Error: 70, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 73, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 78, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 82, count: 2, ratio: 0.05455537370430987%\n",
      "Error: 85, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 89, count: 1, ratio: 0.027277686852154936%\n",
      "Error: 104, count: 2, ratio: 0.05455537370430987%\n",
      "Degree: 55\n",
      "    Error: 0, count: 114, ratio: 71.69811320754717%\n",
      "    Error: 1, count: 20, ratio: 12.578616352201259%\n",
      "    Error: 2, count: 13, ratio: 8.176100628930818%\n",
      "    Error: 3, count: 9, ratio: 5.660377358490566%\n",
      "    Error: 4, count: 1, ratio: 0.6289308176100629%\n",
      "    Error: 6, count: 1, ratio: 0.6289308176100629%\n",
      "    Error: 25, count: 1, ratio: 0.6289308176100629%\n",
      "Degree: 56\n",
      "    Error: 0, count: 49, ratio: 34.751773049645394%\n",
      "    Error: 1, count: 73, ratio: 51.773049645390074%\n",
      "    Error: 2, count: 10, ratio: 7.092198581560283%\n",
      "    Error: 3, count: 3, ratio: 2.127659574468085%\n",
      "    Error: 4, count: 2, ratio: 1.4184397163120568%\n",
      "    Error: 5, count: 2, ratio: 1.4184397163120568%\n",
      "    Error: 14, count: 1, ratio: 0.7092198581560284%\n",
      "    Error: 54, count: 1, ratio: 0.7092198581560284%\n",
      "Degree: 57\n",
      "    Error: 0, count: 51, ratio: 33.116883116883116%\n",
      "    Error: 1, count: 52, ratio: 33.76623376623377%\n",
      "    Error: 2, count: 32, ratio: 20.77922077922078%\n",
      "    Error: 3, count: 12, ratio: 7.792207792207792%\n",
      "    Error: 4, count: 1, ratio: 0.6493506493506493%\n",
      "    Error: 5, count: 2, ratio: 1.2987012987012987%\n",
      "    Error: 8, count: 1, ratio: 0.6493506493506493%\n",
      "    Error: 11, count: 1, ratio: 0.6493506493506493%\n",
      "    Error: 63, count: 1, ratio: 0.6493506493506493%\n",
      "    Error: 70, count: 1, ratio: 0.6493506493506493%\n",
      "Degree: 58\n",
      "    Error: 0, count: 60, ratio: 35.714285714285715%\n",
      "    Error: 1, count: 61, ratio: 36.30952380952381%\n",
      "    Error: 2, count: 24, ratio: 14.285714285714286%\n",
      "    Error: 3, count: 14, ratio: 8.333333333333334%\n",
      "    Error: 4, count: 4, ratio: 2.380952380952381%\n",
      "    Error: 22, count: 1, ratio: 0.5952380952380952%\n",
      "    Error: 69, count: 3, ratio: 1.7857142857142858%\n",
      "    Error: 82, count: 1, ratio: 0.5952380952380952%\n",
      "Degree: 59\n",
      "    Error: 0, count: 81, ratio: 42.1875%\n",
      "    Error: 1, count: 51, ratio: 26.5625%\n",
      "    Error: 2, count: 38, ratio: 19.791666666666668%\n",
      "    Error: 3, count: 17, ratio: 8.854166666666666%\n",
      "    Error: 4, count: 2, ratio: 1.0416666666666667%\n",
      "    Error: 6, count: 1, ratio: 0.5208333333333334%\n",
      "    Error: 68, count: 1, ratio: 0.5208333333333334%\n",
      "    Error: 69, count: 1, ratio: 0.5208333333333334%\n",
      "Degree: 60\n",
      "    Error: 0, count: 67, ratio: 33.004926108374384%\n",
      "    Error: 1, count: 80, ratio: 39.40886699507389%\n",
      "    Error: 2, count: 20, ratio: 9.852216748768473%\n",
      "    Error: 3, count: 21, ratio: 10.344827586206897%\n",
      "    Error: 4, count: 5, ratio: 2.4630541871921183%\n",
      "    Error: 5, count: 6, ratio: 2.955665024630542%\n",
      "    Error: 60, count: 1, ratio: 0.49261083743842365%\n",
      "    Error: 67, count: 2, ratio: 0.9852216748768473%\n",
      "    Error: 68, count: 1, ratio: 0.49261083743842365%\n",
      "Degree: 61\n",
      "    Error: 0, count: 88, ratio: 48.888888888888886%\n",
      "    Error: 1, count: 42, ratio: 23.333333333333332%\n",
      "    Error: 2, count: 26, ratio: 14.444444444444445%\n",
      "    Error: 3, count: 6, ratio: 3.3333333333333335%\n",
      "    Error: 4, count: 14, ratio: 7.777777777777778%\n",
      "    Error: 5, count: 2, ratio: 1.1111111111111112%\n",
      "    Error: 66, count: 1, ratio: 0.5555555555555556%\n",
      "    Error: 67, count: 1, ratio: 0.5555555555555556%\n",
      "Degree: 62\n",
      "    Error: 0, count: 92, ratio: 46.231155778894475%\n",
      "    Error: 1, count: 62, ratio: 31.155778894472363%\n",
      "    Error: 2, count: 21, ratio: 10.552763819095478%\n",
      "    Error: 3, count: 17, ratio: 8.542713567839195%\n",
      "    Error: 4, count: 4, ratio: 2.0100502512562812%\n",
      "    Error: 5, count: 2, ratio: 1.0050251256281406%\n",
      "    Error: 6, count: 1, ratio: 0.5025125628140703%\n",
      "Degree: 63\n",
      "    Error: 0, count: 67, ratio: 37.43016759776536%\n",
      "    Error: 1, count: 45, ratio: 25.139664804469273%\n",
      "    Error: 2, count: 45, ratio: 25.139664804469273%\n",
      "    Error: 3, count: 12, ratio: 6.70391061452514%\n",
      "    Error: 4, count: 5, ratio: 2.793296089385475%\n",
      "    Error: 5, count: 2, ratio: 1.1173184357541899%\n",
      "    Error: 17, count: 1, ratio: 0.5586592178770949%\n",
      "    Error: 47, count: 1, ratio: 0.5586592178770949%\n",
      "    Error: 82, count: 1, ratio: 0.5586592178770949%\n",
      "Degree: 64\n",
      "    Error: 0, count: 29, ratio: 39.189189189189186%\n",
      "    Error: 1, count: 24, ratio: 32.432432432432435%\n",
      "    Error: 2, count: 9, ratio: 12.162162162162161%\n",
      "    Error: 3, count: 10, ratio: 13.513513513513514%\n",
      "    Error: 4, count: 2, ratio: 2.7027027027027026%\n",
      "Degree: 65\n",
      "    Error: 0, count: 116, ratio: 52.48868778280543%\n",
      "    Error: 1, count: 39, ratio: 17.647058823529413%\n",
      "    Error: 2, count: 34, ratio: 15.384615384615385%\n",
      "    Error: 3, count: 14, ratio: 6.334841628959276%\n",
      "    Error: 4, count: 2, ratio: 0.9049773755656109%\n",
      "    Error: 5, count: 11, ratio: 4.97737556561086%\n",
      "    Error: 6, count: 1, ratio: 0.45248868778280543%\n",
      "    Error: 7, count: 1, ratio: 0.45248868778280543%\n",
      "    Error: 10, count: 1, ratio: 0.45248868778280543%\n",
      "    Error: 25, count: 1, ratio: 0.45248868778280543%\n",
      "    Error: 30, count: 1, ratio: 0.45248868778280543%\n",
      "Degree: 66\n",
      "    Error: 0, count: 118, ratio: 58.12807881773399%\n",
      "    Error: 1, count: 64, ratio: 31.527093596059114%\n",
      "    Error: 2, count: 7, ratio: 3.4482758620689653%\n",
      "    Error: 3, count: 10, ratio: 4.926108374384237%\n",
      "    Error: 4, count: 1, ratio: 0.49261083743842365%\n",
      "    Error: 5, count: 2, ratio: 0.9852216748768473%\n",
      "    Error: 7, count: 1, ratio: 0.49261083743842365%\n",
      "Degree: 67\n",
      "    Error: 0, count: 135, ratio: 60.810810810810814%\n",
      "    Error: 1, count: 51, ratio: 22.972972972972972%\n",
      "    Error: 2, count: 26, ratio: 11.711711711711711%\n",
      "    Error: 3, count: 5, ratio: 2.2522522522522523%\n",
      "    Error: 4, count: 3, ratio: 1.3513513513513513%\n",
      "    Error: 5, count: 1, ratio: 0.45045045045045046%\n",
      "    Error: 78, count: 1, ratio: 0.45045045045045046%\n",
      "Degree: 68\n",
      "    Error: 0, count: 105, ratio: 55.851063829787236%\n",
      "    Error: 1, count: 52, ratio: 27.659574468085108%\n",
      "    Error: 2, count: 22, ratio: 11.702127659574469%\n",
      "    Error: 3, count: 6, ratio: 3.1914893617021276%\n",
      "    Error: 4, count: 1, ratio: 0.5319148936170213%\n",
      "    Error: 27, count: 2, ratio: 1.0638297872340425%\n",
      "Degree: 69\n",
      "    Error: 0, count: 58, ratio: 33.52601156069364%\n",
      "    Error: 1, count: 90, ratio: 52.02312138728324%\n",
      "    Error: 2, count: 17, ratio: 9.826589595375722%\n",
      "    Error: 3, count: 5, ratio: 2.8901734104046244%\n",
      "    Error: 4, count: 2, ratio: 1.1560693641618498%\n",
      "    Error: 21, count: 1, ratio: 0.5780346820809249%\n",
      "Degree: 70\n",
      "    Error: 0, count: 102, ratio: 61.07784431137725%\n",
      "    Error: 1, count: 25, ratio: 14.970059880239521%\n",
      "    Error: 2, count: 14, ratio: 8.383233532934131%\n",
      "    Error: 3, count: 4, ratio: 2.395209580838323%\n",
      "    Error: 5, count: 15, ratio: 8.982035928143713%\n",
      "    Error: 12, count: 2, ratio: 1.1976047904191616%\n",
      "    Error: 15, count: 2, ratio: 1.1976047904191616%\n",
      "    Error: 20, count: 2, ratio: 1.1976047904191616%\n",
      "    Error: 25, count: 1, ratio: 0.5988023952095808%\n",
      "Degree: 75\n",
      "    Error: 0, count: 10, ratio: 35.714285714285715%\n",
      "    Error: 5, count: 9, ratio: 32.142857142857146%\n",
      "    Error: 6, count: 1, ratio: 3.5714285714285716%\n",
      "    Error: 10, count: 6, ratio: 21.428571428571427%\n",
      "    Error: 18, count: 1, ratio: 3.5714285714285716%\n",
      "    Error: 20, count: 1, ratio: 3.5714285714285716%\n",
      "Degree: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Error: 0, count: 14, ratio: 36.8421052631579%\n",
      "    Error: 5, count: 11, ratio: 28.94736842105263%\n",
      "    Error: 10, count: 5, ratio: 13.157894736842104%\n",
      "    Error: 13, count: 1, ratio: 2.6315789473684212%\n",
      "    Error: 15, count: 6, ratio: 15.789473684210526%\n",
      "    Error: 17, count: 1, ratio: 2.6315789473684212%\n",
      "Degree: 85\n",
      "    Error: 0, count: 19, ratio: 57.57575757575758%\n",
      "    Error: 5, count: 8, ratio: 24.242424242424242%\n",
      "    Error: 10, count: 4, ratio: 12.121212121212121%\n",
      "    Error: 30, count: 2, ratio: 6.0606060606060606%\n",
      "Degree: 90\n",
      "    Error: 0, count: 12, ratio: 46.15384615384615%\n",
      "    Error: 5, count: 10, ratio: 38.46153846153846%\n",
      "    Error: 10, count: 1, ratio: 3.8461538461538463%\n",
      "    Error: 25, count: 1, ratio: 3.8461538461538463%\n",
      "    Error: 35, count: 2, ratio: 7.6923076923076925%\n",
      "Degree: 95\n",
      "    Error: 0, count: 22, ratio: 73.33333333333333%\n",
      "    Error: 5, count: 5, ratio: 16.666666666666668%\n",
      "    Error: 10, count: 1, ratio: 3.3333333333333335%\n",
      "    Error: 33, count: 1, ratio: 3.3333333333333335%\n",
      "    Error: 36, count: 1, ratio: 3.3333333333333335%\n",
      "Degree: 110\n",
      "    Error: 0, count: 60, ratio: 92.3076923076923%\n",
      "    Error: 5, count: 5, ratio: 7.6923076923076925%\n",
      "Degree: 115\n",
      "    Error: 0, count: 65, ratio: 86.66666666666667%\n",
      "    Error: 5, count: 8, ratio: 10.666666666666666%\n",
      "    Error: 20, count: 1, ratio: 1.3333333333333333%\n",
      "    Error: 59, count: 1, ratio: 1.3333333333333333%\n",
      "Degree: 120\n",
      "    Error: 0, count: 44, ratio: 78.57142857142857%\n",
      "    Error: 5, count: 9, ratio: 16.071428571428573%\n",
      "    Error: 25, count: 2, ratio: 3.5714285714285716%\n",
      "    Error: 45, count: 1, ratio: 1.7857142857142858%\n",
      "Degree: 125\n",
      "    Error: 0, count: 35, ratio: 70.0%\n",
      "    Error: 1, count: 5, ratio: 10.0%\n",
      "    Error: 3, count: 2, ratio: 4.0%\n",
      "    Error: 5, count: 6, ratio: 12.0%\n",
      "    Error: 15, count: 1, ratio: 2.0%\n",
      "    Error: 40, count: 1, ratio: 2.0%\n",
      "Degree: 126\n",
      "    Error: 0, count: 11, ratio: 40.74074074074074%\n",
      "    Error: 1, count: 9, ratio: 33.333333333333336%\n",
      "    Error: 2, count: 7, ratio: 25.925925925925927%\n",
      "Degree: 127\n",
      "    Error: 0, count: 97, ratio: 80.83333333333333%\n",
      "    Error: 1, count: 22, ratio: 18.333333333333332%\n",
      "    Error: 67, count: 1, ratio: 0.8333333333333334%\n",
      "Degree: 128\n",
      "    Error: 0, count: 98, ratio: 85.96491228070175%\n",
      "    Error: 1, count: 14, ratio: 12.280701754385966%\n",
      "    Error: 2, count: 2, ratio: 1.7543859649122806%\n",
      "Degree: 130\n",
      "    Error: 0, count: 25, ratio: 67.56756756756756%\n",
      "    Error: 5, count: 8, ratio: 21.62162162162162%\n",
      "    Error: 10, count: 1, ratio: 2.7027027027027026%\n",
      "    Error: 35, count: 1, ratio: 2.7027027027027026%\n",
      "    Error: 40, count: 1, ratio: 2.7027027027027026%\n",
      "    Error: 73, count: 1, ratio: 2.7027027027027026%\n",
      "Degree: 135\n",
      "    Error: 0, count: 20, ratio: 80.0%\n",
      "    Error: 5, count: 5, ratio: 20.0%\n",
      "Degree: 140\n",
      "    Error: 0, count: 22, ratio: 81.48148148148148%\n",
      "    Error: 5, count: 2, ratio: 7.407407407407407%\n",
      "    Error: 20, count: 2, ratio: 7.407407407407407%\n",
      "    Error: 70, count: 1, ratio: 3.7037037037037037%\n",
      "Degree: 145\n",
      "    Error: 0, count: 17, ratio: 85.0%\n",
      "    Error: 5, count: 2, ratio: 10.0%\n",
      "    Error: 10, count: 1, ratio: 5.0%\n",
      "Degree: 150\n",
      "    Error: 0, count: 12, ratio: 63.1578947368421%\n",
      "    Error: 5, count: 6, ratio: 31.57894736842105%\n",
      "    Error: 85, count: 1, ratio: 5.2631578947368425%\n",
      "Degree: 155\n",
      "    Error: 0, count: 10, ratio: 50.0%\n",
      "    Error: 5, count: 7, ratio: 35.0%\n",
      "    Error: 10, count: 1, ratio: 5.0%\n",
      "    Error: 30, count: 1, ratio: 5.0%\n",
      "    Error: 89, count: 1, ratio: 5.0%\n",
      "Degree: 160\n",
      "    Error: 0, count: 7, ratio: 46.666666666666664%\n",
      "    Error: 5, count: 6, ratio: 40.0%\n",
      "    Error: 104, count: 2, ratio: 13.333333333333334%\n",
      "Degree: 165\n",
      "    Error: 0, count: 13, ratio: 72.22222222222223%\n",
      "    Error: 5, count: 5, ratio: 27.77777777777778%\n",
      "total samples: 3666\n"
     ]
    }
   ],
   "source": [
    "#!/Users/caiyunxiang/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#dataset_path = '/Users/caiyunxiang/Desktop/AOA/c25_self'\n",
    "#dataset_path = '/Users/caiyunxiang/Desktop/Self/Self_result/offset_np'\n",
    "dataset_path = '/Users/caiyunxiang/Desktop/Self/Self_result/offset_np'\n",
    "#dataset_path = '/Users/caiyunxiang/Desktop/AOA/6-15-result/offset_np'\n",
    "#dataset_path = '/Users/caiyunxiang/Desktop/AOA/c2'\n",
    "#dataset_path = '/Users/caiyunxiang/Desktop/FUCK/c25/union'\n",
    "model_path = '/Users/caiyunxiang/Desktop/Self/Self_result/Model'\n",
    "batch_size = 5000\n",
    "test_batch_size = 2392\n",
    "hidden_layer = 6\n",
    "Input_dim, H_1, H_2, H_3, H_4, H_5, H_6, Output_dim = 312, 256, 224, 160, 128, 82, 64, 36\n",
    "Learning_rate = 1e-3\n",
    "epoch = 100\n",
    "cnn = True\n",
    "cnn2d = False\n",
    "drop_p = 0.5\n",
    "print_iter = 50\n",
    "norm = True\n",
    "\n",
    "class ArgOffsetDataset(Dataset):\n",
    "    \"\"\"CSI argument offset dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, datatype='train'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.data_path = os.path.join(self.dataset_path, os.path.join(datatype, 'data.npy'))\n",
    "        self.labels_path = os.path.join(self.dataset_path, os.path.join(datatype, 'labels.npy'))\n",
    "        self.data = np.load(self.data_path)\n",
    "        #if norm:\n",
    "            #self.data = self.__z_score__(self.data)\n",
    "        if cnn:\n",
    "            if cnn2d:\n",
    "                self.data = np.reshape(self.data, (self.data.shape[0], 1, 6, 52))\n",
    "            else:\n",
    "                self.data = np.reshape(self.data, (self.data.shape[0], 1, self.data.shape[1]))\n",
    "        self.labels = np.load(self.labels_path)\n",
    "        print(datatype+\"_data shape: \", self.data.shape)\n",
    "        print(datatype+\"_labels shape: \", self.labels.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return self.train_data[idx], self.train_labels[idx]\n",
    "        sam = torch.from_numpy(self.data[idx]).type(torch.float32)\n",
    "        lab = torch.from_numpy(self.labels[idx]).type(torch.long)\n",
    "        # return self.test_data[idx], self.test_labels[idx]\n",
    "        return sam, lab\n",
    "    \n",
    "    def __norm__(self, dataset):\n",
    "        print(f'normalization...')\n",
    "        return dataset/np.std(dataset,axis=0)\n",
    "\n",
    "    def __norm2__(self, dataset):\n",
    "        print(f'normalization...')\n",
    "        return (dataset-np.min(dataset))/(np.max(dataset)-np.min(dataset))\n",
    "\n",
    "    def __z_score__(self, dataset):\n",
    "        print(f'z-scoring...')\n",
    "        dataset -= np.mean(dataset, axis=0)\n",
    "        dataset /= np.std(dataset, axis=0)\n",
    "        return dataset\n",
    "\n",
    "class AoACNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AoACNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1), # input [batchsize,1,312], output [batchsize,8,156]\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=3, stride=3) # input [batchsize,8,156], output [batchsize,8,52]\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1), # input [batchsize,8,52], output [batchsize,16, 26]\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool1d(kernel_size=2, stride=2) # input [batchsize,16,26], output [batchsize,16,13]\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1), # input [batchsize,16,13], output [batchsize,32, 13]\n",
    "            torch.nn.BatchNorm1d(32), \n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*13, 100),\n",
    "            # torch.nn.Dropout(drop_p),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, Output_dim)\n",
    "        )\n",
    "        # self.dp = torch.nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        # x = self.dp(x)\n",
    "        return x\n",
    "\n",
    "class AoACNN_2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AoACNN_2d, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1), # input [batchsize,1,6,52], output [batchsize,8,3,26]\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool2d(kernel_size=3, stride=3) # input [batchsize,8,156], output [batchsize,8,52]\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=(0,1)), # input [batchsize,8,3,26], output [batchsize,16,1,13]\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool2d(kernel_size=2, stride=2) # input [batchsize,16,26], output [batchsize,16,13]\n",
    "        )\n",
    "        # self.conv3 = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1), # input [batchsize,16,13], output [batchsize,32, 13]\n",
    "        #     torch.nn.BatchNorm2d(32), \n",
    "        #     torch.nn.ReLU(),\n",
    "        # )\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16*13, 100),\n",
    "            torch.nn.Dropout(drop_p),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, Output_dim)\n",
    "        )\n",
    "        # self.dp = torch.nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        # x = self.dp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def work(mode='Train', cnn=False):\n",
    "    train_dataset = ArgOffsetDataset(dataset_path, 'train')\n",
    "    test_dataset = ArgOffsetDataset(dataset_path, 'test')\n",
    "    trainDataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    testDataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "    model_type = \"\"\n",
    "    if mode=='Train':\n",
    "        if cnn:\n",
    "            if cnn2d:\n",
    "                model = AoACNN_2d()\n",
    "                model_type = 'CNN2d'\n",
    "            else:\n",
    "                model = AoACNN()\n",
    "                model_type = 'CNN1d'\n",
    "        else:\n",
    "            model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(Input_dim, H_1),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H_1, H_2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H_2, H_3),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H_3, H_4),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H_4, H_5),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H_5, H_6),\n",
    "                torch.nn.ReLU(),\n",
    "                # torch.nn.Linear(H_6, H_7),\n",
    "                # torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H_6, Output_dim),\n",
    "            )\n",
    "            model_type = 'FCN'\n",
    "        if norm:\n",
    "            model_name = f'model_{model_type}_hidden-layer_{hidden_layer}_epoch_{epoch}_lr_{Learning_rate}_norm.pt'\n",
    "        else:\n",
    "            model_name = f'model_{model_type}_hidden-layer_{hidden_layer}_epoch_{epoch}_lr_{Learning_rate}.pt'\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        learning_rate = Learning_rate\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "        # optimizer = torch.optim.Adagrad()\n",
    "        loss_count = []\n",
    "        loss_thr = None\n",
    "        for iter in range(epoch):\n",
    "            # if loss_thr != None:\n",
    "            #     if loss_thr <= 0.7 and loss_thr > 0.6 and learning_rate == Learning_rate:\n",
    "            #         learning_rate /= 5\n",
    "            #         print('new learing rate: ', learning_rate)\n",
    "            #         for param_groups in optimizer.param_groups:\n",
    "            #             param_groups['lr'] = learning_rate\n",
    "            #     elif loss_thr <= 0.6 and loss_thr > 0.5 and learning_rate == Lsearning_rate/5:\n",
    "            #         learning_rate /= 10\n",
    "            #         print('new learing rate: ', learning_rate)\n",
    "            #         for param_groups in optimizer.param_groups:\n",
    "            #             param_groups['lr'] = learning_rate\n",
    "            #     elif loss_thr <= 0.5 and loss_thr > 0.1 and learning_rate == Learning_rate/50:\n",
    "            #         learning_rate /= 10\n",
    "            #         print('new learing rate: ', learning_rate)\n",
    "            #         for param_groups in optimizer.param_groups:\n",
    "            #             param_groups['lr'] = learning_rate\n",
    "            #     elif loss_thr <= 0.1:\n",
    "            #         print('training finished.')\n",
    "            #         break\n",
    "            if loss_thr != None:\n",
    "                if loss_thr <= 0.1:\n",
    "                    print('training finished.')\n",
    "                    break\n",
    "\n",
    "            print('Epoch ', iter, ': ')\n",
    "            cnt = 0\n",
    "            loss_sum = 0\n",
    "            for i, (data_batch, labels_batch) in enumerate(trainDataloader):\n",
    "                # print(i, data_batch, data_batch.shape, labels_batch, labels_batch.shape)\n",
    "                # print('Batch ', i, data_batch.shape, labels_batch.shape)\n",
    "                # print('Batch ', i)\n",
    "                y_pred = model(data_batch)\n",
    "                loss = loss_fn(y_pred, labels_batch.reshape(-1))\n",
    "                if i%print_iter == 0:\n",
    "                    loss_count.append(loss)\n",
    "                    print(\"Iteration: \", i, \", Loss: \", loss.item())\n",
    "                loss_sum += loss.item()\n",
    "                # model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # with torch.no_grad():\n",
    "                #     for param in model.parameters():\n",
    "                #         param -= learning_rate * param.grad\n",
    "                optimizer.step()\n",
    "                cnt = i\n",
    "            loss_thr = loss_sum/(cnt+1)\n",
    "\n",
    "            torch.save(model, os.path.join(model_path, model_name))\n",
    "            for (test_x, test_y) in testDataloader:\n",
    "                pred = model(test_x)\n",
    "                # print(pred)\n",
    "                acc = np.argmax(pred.detach().numpy(), axis=1) == test_y.detach().numpy().reshape(-1)\n",
    "                # pred error distribution (degrees)\n",
    "                err = np.argmax(pred.detach().numpy(), axis=1) - test_y.detach().numpy().reshape(-1)\n",
    "                # err_index = \n",
    "                err_index = np.where(err!=0)\n",
    "                err_pred = np.argmax(pred.detach().numpy(), axis=1)[err_index]\n",
    "                err_truelb   = test_y.detach().numpy().reshape(-1)[err_index]\n",
    "                #lb2deg = list(range(55,71))+[110, 115, 120]+list(range(125, 129))\n",
    "                #lb2deg = list(range(55,71))+[110, 115, 120]+list(range(125, 131))\n",
    "                #lb2deg = [i for i in range(65,130,5)]\n",
    "                lb2deg = list(range(55,71))+list(range(75,100,5))+list(range(110,125,5)) +list(range(125,129))+list(range(130,170,5))\n",
    "\n",
    "                \n",
    "                #print('errp: ', [lb2deg[i] for i in err_pred[:30]])\n",
    "                #print('true: ', [lb2deg[i] for i in err_truelb[:30]])\n",
    "\n",
    "\n",
    "                # print(acc)\n",
    "                print(\"Validate accuracy: \", acc.mean())\n",
    "                break\n",
    "        # acc on train dataset\n",
    "        acc_sum = []\n",
    "        for i, (train_x, train_y) in enumerate(trainDataloader):\n",
    "            print('Batch ', i)\n",
    "            pred = model(train_x)\n",
    "            # print('pred: ', pred.shape, 'label: ', train_y.shape)\n",
    "            acc = np.argmax(pred.detach().numpy(), axis=1) == train_y.detach().numpy().reshape(-1)\n",
    "            # pred_l += list(np.argmax(pred.detach().numpy(), axis=1))\n",
    "            # print('pred_l: ', pred_l)\n",
    "            # true_label += list(train_y.detach().numpy().reshape(-1))\n",
    "            acc_sum.append(acc.mean())\n",
    "        print('Total train accuracy: ', sum(acc_sum)/len(acc_sum))\n",
    "        plt.figure('Loss')\n",
    "        plt.plot(loss_count, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if mode=='Test':\n",
    "        if cnn:\n",
    "            if cnn2d:\n",
    "                model_type = 'CNN2d'\n",
    "            else:\n",
    "                model_type = 'CNN1d'\n",
    "        else:\n",
    "            model_type = 'FCN'\n",
    "        if norm:\n",
    "            model_name = f'model_{model_type}_hidden-layer_{hidden_layer}_epoch_{epoch}_lr_{Learning_rate}_norm.pt'\n",
    "        else:\n",
    "            model_name = f'model_{model_type}_hidden-layer_{hidden_layer}_epoch_{epoch}_lr_{Learning_rate}.pt'\n",
    "        model = torch.load(os.path.join(model_path, model_name))\n",
    "        model.eval()\n",
    "        acc_sum = []\n",
    "        pred_l = []\n",
    "        true_label = []\n",
    "\n",
    "        lb2deg = list(range(55,71))+list(range(75,100,5))+list(range(110,125,5)) +list(range(125,129))+list(range(130,170,5))\n",
    "        #lb2deg = list(range(55,71))+[110, 115, 120]+list(range(125, 129))\n",
    "        #lb2deg = list(range(55,71))+[110, 115, 120]+list(range(125, 131))\n",
    "        #lb2deg = [i for i in range(65,130,5)]\n",
    "        for i, (test_x, test_y) in enumerate(testDataloader):\n",
    "            print('Batch ', i)\n",
    "            pred = model(test_x)\n",
    "            # print('pred: ', pred.shape, 'label: ', test_y.shape)\n",
    "            acc = np.argmax(pred.detach().numpy(), axis=1) == test_y.detach().numpy().reshape(-1)\n",
    "            pred_l += list(np.argmax(pred.detach().numpy(), axis=1))\n",
    "            # print('pred_l: ', pred_l)\n",
    "            true_label += list(test_y.detach().numpy().reshape(-1))\n",
    "            acc_sum.append(acc.mean())\n",
    "            print(\"Test accuracy: \", acc.mean())\n",
    "        print('Total test accuracy: ', sum(acc_sum)/len(acc_sum))\n",
    "        # plt.figure('Test accuracy')\n",
    "        # plt.plot(acc_sum, 'o', label='test_accuracy')\n",
    "        # plt.title('ArgOffset Test Accuracy')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "        # error analyse\n",
    "        cnt = len(pred_l)\n",
    "        print(f'total test samples: {cnt}')\n",
    "        # print(f'pred: {pred_l}\\ntrue: {true_label}')\n",
    "        pred_l = np.array(pred_l)\n",
    "        true_label = np.array(true_label)\n",
    "        for i,_ in enumerate(pred_l):\n",
    "            pred_l[i] = lb2deg[pred_l[i]]\n",
    "            true_label[i] = lb2deg[true_label[i]]\n",
    "        pred_err = np.abs(pred_l-true_label)\n",
    "        # print(f'err:{list(pred_err)}')\n",
    "        pred_err_dict = {} # {err_abs: count}\n",
    "        deg_err_dict = {} # {deg: {err_abs:count}}\n",
    "        for p in pred_err:\n",
    "            if p in pred_err_dict.keys():\n",
    "                pred_err_dict[p] += 1\n",
    "            else:\n",
    "                # print(f'error key: {p}')\n",
    "                pred_err_dict[p] = 1\n",
    "\n",
    "        for d in lb2deg:\n",
    "            deg_err_dict[d] = {}\n",
    "            for i in lb2deg:\n",
    "                for j in lb2deg:\n",
    "                    deg_err_dict[d][abs(i-j)] = 0\n",
    "            for i, l in enumerate(true_label):\n",
    "                if l == d:\n",
    "                    deg_err_dict[d][abs(l-pred_l[i])] += 1\n",
    "\n",
    "        pred_rst = sorted(pred_err_dict.items(), key=itemgetter(0))\n",
    "        for e, c in pred_rst:\n",
    "            if c>0:\n",
    "                print(f'Error: {e}, count: {c}, ratio: {c*100/cnt}%')\n",
    "        deg_err_rst = {}\n",
    "        total_all = 0\n",
    "        for d in lb2deg:\n",
    "            print(f'Degree: {d}')\n",
    "            deg_err_rst[d] = sorted(deg_err_dict[d].items(), key=itemgetter(0))\n",
    "            total = 0\n",
    "            for _, c in deg_err_rst[d]:\n",
    "                total += c\n",
    "            for e, c in deg_err_rst[d]:\n",
    "                if c > 0:\n",
    "                    print(f'    Error: {e}, count: {c}, ratio: {c*100/total}%')\n",
    "            total_all += total\n",
    "        # print(deg_err_dict)\n",
    "        print(f'total samples: {total_all}')\n",
    "        #print(test_y)Degree: 110\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #work(mode='Train', cnn=True)\n",
    "    \n",
    "    work(mode='Test', cnn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
